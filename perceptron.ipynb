{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2b23e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are helper functions for plotting the decision boundary\n",
    "def plot_bounds_from_wb(w, b, X, label=None): # This function plots the bounds of the decision boundary with weight vector w and bias b\n",
    "    w1, w2 = w\n",
    "    xs = np.linspace(X[:,0].min()-.5, X[:,0].max()+.5, 200) # This evenly spreads out the points in the x-axis\n",
    "\n",
    "    if abs(w2) > 1e-12:\n",
    "        ys = -(b+w1*xs) / (w2 + 1e-12) # Add a small epsilon to avoid division by zero\n",
    "        plt.plot(xs, ys, label=label) # Plot the line\n",
    "    elif abs(w1) > 1e-12:\n",
    "        x_vert = -b/(w1 + 1e-12) # Calculate the x-coordinate of the vertical line\n",
    "        plt.axvline(x_vert, label=label) # Plot the axis vertical line\n",
    "\n",
    "def plot_boundary_aug(w_aug, X, label=None): # This function plots the boundary of the decision boundary with weight vector w_aug\n",
    "    \"\"\"Augmented boundary for w_aug=[b,w1,w2].\"\"\"\n",
    "    b, w1, w2 = w_aug\n",
    "    xs = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200) # This evenly spreads out the points in the x-axis\n",
    "\n",
    "    if abs(w2) > 1e-12:\n",
    "        ys = -(b + w1*xs)/(w2 + 1e-12)\n",
    "        plt.plot(xs, ys, label=label)\n",
    "    elif abs(w1) > 1e-12:\n",
    "        x_vert = -b/(w1 + 1e-12)\n",
    "        plt.axvline(x_vert, label=label)\n",
    "\n",
    "\n",
    "# ---- Part A: Tiny World ----\n",
    "\"\"\"\n",
    "Discussion:\n",
    "Created a small \n",
    "\"\"\"\n",
    "X = np.array([[1,1], [2,2], [1,2], [-1,-1], [-1,-2], [-2,-2]], dtype=float)\n",
    "y = np.array([1,1,1,-1,-1,-1], dtype=int)\n",
    "\n",
    "w_chosen = np.array([1.0, 1.0])\n",
    "b_chosen = 0.0\n",
    "\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "plt.scatter(X[y==1,0], X[y==1,1], marker='o', label='y=+1')\n",
    "plt.scatter(X[y==-1,0], X[y==-1,1], marker='x', label='y=-1')\n",
    "plot_bounds_from_wb(w_chosen, b_chosen, X, label='chosen (w,b)')\n",
    "plt.xlabel('x1'); plt.ylabel('x2'); plt.grid(True); plt.legend(); plt.title('Part A: Tiny World')\n",
    "plt.show()\n",
    "\n",
    "# ---- Part B: Perceptron from Scratch ----\n",
    "\"\"\"\n",
    "Discussion:\n",
    "Created Perceptron from scratch. It starts with a random weight and visits each point in the dataset one by one. \n",
    "At every step it checks if the point is on the wrong side of the hyperplane by using g(x) = b + w1*x1 + w2*x2.\n",
    "If the point is wrong or the boundary of the hyperplane is not correct, it updates the weight and bias.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Augmented weights: self.w = [b, w1, w2]\n",
    "    We record a frame after EVERY sample visit (even if no update).\n",
    "    history[k] contains:\n",
    "      - 'w': weights *after* processing that sample (None for the initial frame),\n",
    "      - 'i': index of the sample processed on this frame (-1 for initial),\n",
    "      - 'seen': how many samples have been seen so far in this pass (for axis bounds),\n",
    "      - 'updated': whether a mistake update happened.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs = 2, lr = .1, seed = 0):\n",
    "        self.lr = float(lr)\n",
    "        rng = np.random.default_rng(seed) # The argument seed makes the randomness reproducible.\n",
    "        self.w = rng.standard_normal(num_inputs + 1) # [b, w1, w2]\n",
    "        self.history = []\n",
    "\n",
    "    def score(self, x):\n",
    "        return self.w[0] +self.w[1:].dot(x) # self.w[0] = b, self.w[1:] = [w1, w2] so self.w[0] + self.w[1:].dot(x) = b + w1*x1 + w2*x2\n",
    "\n",
    "    def predict(self, x):\n",
    "        return 1 if self.score(x) > 0 else -1 # This is the prediction rule for the perceptron\n",
    "\n",
    "    def fit(self, X, y, epochs=1):\n",
    "        self.history = []\n",
    "        for _ in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                g = self.score(X[i]) # This is the score of the perceptron\n",
    "                updated = False\n",
    "                if y[i] * g <= 0:                 # mistake (or on boundary)\n",
    "                    self.w[1:] += self.lr * y[i] * X[i]\n",
    "                    self.w[0]  += self.lr * y[i]\n",
    "                    updated = True\n",
    "                    # record after visiting this sample\n",
    "                self.history.append({\"w\": self.w.copy(), \"i\": i, \"updated\": updated})\n",
    "            return self\n",
    "\n",
    "p = Perceptron(num_inputs=2, lr=1.0, seed=0)\n",
    "p.fit(X, y)\n",
    "print(\"Final weights [b,w1,w2] after one pass:\", np.round(p.w, 4))\n",
    "print(\"Total mistakes (frames):\", len(p.history))\n",
    "print(\"Total updates:\", sum(fr[\"updated\"] for fr in p.history))\n",
    "\n",
    "# Plotting the history\n",
    "for k, fr in enumerate(p.history, start=1): \n",
    "    i = fr[\"i\"]\n",
    "    w_aug = fr[\"w\"]\n",
    "\n",
    "    plt.figure(figsize=(5.5,5.5))\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], alpha=0.5, label=\"points\")\n",
    "\n",
    "    X_seen, y_seen = X[:i+1], y[:i+1]\n",
    "\n",
    "    plt.scatter(X_seen[y_seen == 1,0], X_seen[y_seen == 1, 1], marker='o', label='y=+1 seen') # Markers for positive samples\n",
    "    plt.scatter(X_seen[y_seen == -1,0], X_seen[y_seen == -1, 1], marker='x', label='y=-1 seen') # Markers for negative samples\n",
    "\n",
    "    plt.scatter([X[i,0]],[X[i,1]], s=140, facecolors=\"none\", edgecolors=\"tab:red\", linewidths=2, label=\"current\") # Current sample\n",
    "    if i+1 < len(X):\n",
    "        j = i+1\n",
    "        plt.scatter([X[j,0]],[X[j,1]], s=140, facecolors=\"none\", edgecolors=\"gold\", linewidths=2, label=\"next\")\n",
    "    # boundary + weight vector\n",
    "    plot_boundary_aug(w_aug, X, label=\"boundary\")\n",
    "    b, w1, w2 = w_aug\n",
    "    plt.arrow(0,0,w1,w2,head_width=0.15,length_includes_head=True)\n",
    "    plt.text(w1,w2,\"  w\",va=\"center\")\n",
    "\n",
    "    # This adjusts the x and y axis to the minimum and maximum of the seen samples\n",
    "    xmin, xmax = X_seen[:,0].min(), X_seen[:,0].max()\n",
    "    ymin, ymax = X_seen[:,1].min(), X_seen[:,1].max()\n",
    "    pad_x = 0.5 * max(1e-6, xmax - xmin)\n",
    "    pad_y = 0.5 * max(1e-6, ymax - ymin)\n",
    "    plt.xlim(xmin - pad_x, xmax + pad_x)\n",
    "    plt.ylim(ymin - pad_y, ymax + pad_y)\n",
    "\n",
    "    tag = \" (updated)\" if fr[\"updated\"] else \" (no update)\"\n",
    "    plt.title(f\"Step {k}: after visiting sample {i}{tag}\")\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# ---- Part C: Bound vs. Reality ----\n",
    "\"\"\"\n",
    "Discussion:\n",
    "Bound vs Reality looks at how the perceptron;s performace comapres to its limits. \n",
    "The theorical formula says that k <= (R/𝛾)**2.\n",
    "Based on what I computed on both caluses the real number of mistakes was smaller than the bound, \n",
    "which means that the bound is just a guarantee not an exact count.\n",
    "This means that the perceptron is performing well and is not overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# R = max_i || [x_i ; 1] || 2\n",
    "\n",
    "X_aug = np.hstack([np.ones((X.shape[0], 1)), X]) \n",
    "R = np.linalg.norm(X_aug, axis=1).max()\n",
    "\n",
    "wstar_aug = np.array([b_chosen, *w_chosen])\n",
    "norm_wstar = np.linalg.norm(wstar_aug)\n",
    "margins = y * (X_aug @ wstar_aug) / (norm_wstar + 1e-12)\n",
    "gamma = margins.min()\n",
    "\n",
    "k_mistakes = len(p.history)\n",
    "bound = (R / (gamma + 1e-12))**2\n",
    "\n",
    "print(f\"R = {R:.4f}\")\n",
    "print(f\"gamma (from chosen w*) = {gamma:.4f}\")\n",
    "print(f\"(R/gamma)^2 = {bound:.4f}\")\n",
    "print(f\"Actual mistakes k = {k_mistakes}\")\n",
    "\n",
    "# ---- Part D: When it Fails (XOR) + Feature Map Φ ----\n",
    "\"\"\"\n",
    "Discussion:\n",
    "XOR is a dataset that is not linearly separable. \n",
    "It is beacuse the Perceptron never full conveges and it keeps udating the weights again and again.\n",
    "This is why the Perceptron never converges on the XOR dataset.\n",
    "So basically the Perceptron can only work on linearly separable datasets.\n",
    "\"\"\"\n",
    "\n",
    "X_xor = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "y_xor = np.array([-1, +1, +1, -1], dtype=int)\n",
    "\n",
    "X_xor = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "y_xor = np.array([-1, +1, +1, -1], dtype=int)\n",
    "\n",
    "# Show cycling (keeps making mistakes)\n",
    "p_xor = Perceptron(num_inputs=2, lr=1.0, seed=1)\n",
    "mistakes = 0\n",
    "for epoch in range(10):  # a few epochs just to show it doesn't converge\n",
    "    any_mistake = False\n",
    "    for i in range(len(X_xor)):\n",
    "        g = p_xor.score(X_xor[i])\n",
    "        if y_xor[i]*g <= 0:\n",
    "            p_xor.w[1:] += p_xor.lr*y_xor[i]*X_xor[i]\n",
    "            p_xor.w[0]  += p_xor.lr*y_xor[i]\n",
    "            mistakes += 1\n",
    "            any_mistake = True\n",
    "print(\"XOR mistakes across a few epochs (should be > 0):\", mistakes)\n",
    "\n",
    "# Polynomial feature map Φ(x) = [x1, x2, x1*x2, x1^2, x2^2]\n",
    "def Phi(X):\n",
    "    x1 = X[:,0]; x2 = X[:,1]\n",
    "    return np.c_[x1, x2, x1*x2, x1**2, x2**2]\n",
    "\n",
    "Xp = Phi(X_xor)\n",
    "p_phi = Perceptron(num_inputs=Xp.shape[1], lr=1.0, seed=0)\n",
    "\n",
    "def fit_until_converge(percep, Xf, y, max_epochs=100):\n",
    "    for _ in range(max_epochs):\n",
    "        errs = 0\n",
    "        for i in range(len(Xf)):\n",
    "            g = percep.w[0] + percep.w[1:].dot(Xf[i])\n",
    "            if y[i]*g <= 0:\n",
    "                percep.w[1:] += percep.lr*y[i]*Xf[i]\n",
    "                percep.w[0]  += percep.lr*y[i]\n",
    "                errs += 1\n",
    "        if errs == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "converged = fit_until_converge(p_phi, Xp, y_xor, max_epochs=100)\n",
    "print(\"Perceptron in Φ-space converged:\", converged)\n",
    "\n",
    "# Visualize implied nonlinear boundary back in original 2D: w^T Φ(x) + b = 0\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5,1.5,300), np.linspace(-0.5,1.5,300))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = p_phi.w[0] + Phi(grid) @ p_phi.w[1:]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2)  # decision contour\n",
    "plt.scatter(X_xor[y_xor==1,0], X_xor[y_xor==1,1], marker='o', label='y=+1')\n",
    "plt.scatter(X_xor[y_xor==-1,0], X_xor[y_xor==-1,1], marker='x', label='y=-1')\n",
    "plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n",
    "plt.xlabel('x1'); plt.ylabel('x2'); plt.grid(True); plt.legend()\n",
    "plt.title('Part D: Φ-space boundary mapped back to 2D')\n",
    "plt.show()\n",
    "\n",
    "# ---------- Part E: Scaling ----------\n",
    "\"\"\"\n",
    "Discussion:\n",
    "In this part I scaled the input size of R, which showed how many updates are needed to converge.\n",
    "Even though the shape of the data stays the same, larger values  can make learning slower because updates become larger in size.\n",
    "This part shows that the perceptron’s speed and stability depend on how the input data is scaled.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Standardize features (z-score) column-wise\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std  = X.std(axis=0) + 1e-12\n",
    "X_stdzd = (X - X_mean) / X_std\n",
    "\n",
    "# Recompute R, gamma on standardized data using a simple chosen direction\n",
    "w_chosen_std = np.array([1.0, 1.0])\n",
    "b_chosen_std = 0.0\n",
    "X_aug_std = np.hstack([np.ones((X_stdzd.shape[0],1)), X_stdzd])\n",
    "w_star_aug_std = np.array([b_chosen_std, *w_chosen_std])\n",
    "R_std = np.linalg.norm(X_aug_std, axis=1).max()\n",
    "gamma_std = (y * (X_aug_std @ w_star_aug_std) / (np.linalg.norm(w_star_aug_std)+1e-12)).min()\n",
    "\n",
    "# Train perceptron on standardized data (one pass) to compare mistake count\n",
    "p_std = Perceptron(num_inputs=2, lr=1.0, seed=0)\n",
    "p_std.fit(X_stdzd, y)\n",
    "\n",
    "print(f\"Unscaled:     R={R:.4f},  gamma={gamma:.4f},  (R/gamma)^2={(R/(gamma+1e-12))**2:.4f},  mistakes={k_mistakes}\")\n",
    "print(f\"Standardized: R={R_std:.4f}, gamma={gamma_std:.4f}, (R/gamma)^2={(R_std/(gamma_std+1e-12))**2:.4f}, mistakes={len(p_std.history)}\")\n",
    "\n",
    "# Final boundary in standardized space\n",
    "plt.figure(figsize=(5.5,5.5))\n",
    "plt.scatter(X_stdzd[y==1,0], X_stdzd[y==1,1], marker='o', label='y=+1 (std)')\n",
    "plt.scatter(X_stdzd[y==-1,0], X_stdzd[y==-1,1], marker='x', label='y=-1 (std)')\n",
    "plot_boundary_aug(p_std.w, X_stdzd, label='perceptron (std)')\n",
    "plt.xlabel('x1 (std)'); plt.ylabel('x2 (std)'); plt.grid(True); plt.legend()\n",
    "plt.title('Part E: Final boundary on standardized data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
